{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCR/k2jqQeDZg13iAOE5iA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b4a8555638046eda8e0fbe8921646ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3123836d8c14133afbb73a9ae8d5afa",
              "IPY_MODEL_484197db8b8b4128b774d0a40f70c3ba",
              "IPY_MODEL_ce356159b76d4871a9a7647e4c80f595"
            ],
            "layout": "IPY_MODEL_ee772066f9c2412eb0e37cd7904a2344"
          }
        },
        "f3123836d8c14133afbb73a9ae8d5afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b821eb42a7124be0951a4f4fbba53b43",
            "placeholder": "​",
            "style": "IPY_MODEL_25328223f7dd4470907cd7b621deecf5",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "484197db8b8b4128b774d0a40f70c3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc39386c6e9a4f0ba95fa94dcb2ae981",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccd3a104f7f5455dae20e8ffe7d4e364",
            "value": 5
          }
        },
        "ce356159b76d4871a9a7647e4c80f595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_533bfe864e0a4029a06e48de39486675",
            "placeholder": "​",
            "style": "IPY_MODEL_51712b12e12b486a9e539c5e60244c86",
            "value": " 5/5 [00:21&lt;00:00,  5.47s/it]"
          }
        },
        "ee772066f9c2412eb0e37cd7904a2344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b821eb42a7124be0951a4f4fbba53b43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25328223f7dd4470907cd7b621deecf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc39386c6e9a4f0ba95fa94dcb2ae981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd3a104f7f5455dae20e8ffe7d4e364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "533bfe864e0a4029a06e48de39486675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51712b12e12b486a9e539c5e60244c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/promptparty/blob/main/datageneration/BackgroundDataAugumentationforAnswerImage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "sHtNHs_cQjar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LXPjx_9QY6L"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "qE5gVYeygm7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix Point"
      ],
      "metadata": {
        "id": "5Uj5NADRQis3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please change this line in /usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py\n",
        "# 1085, 1086\n",
        "# after changing, please restart\n",
        "\"\"\"\n",
        "# class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n",
        "# masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n",
        "class_queries_logits = outputs.transformer_decoder_class_predictions\n",
        "masks_queries_logits = outputs.transformer_decoder_mask_predictions\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zqh7eEw4QiUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo data"
      ],
      "metadata": {
        "id": "PPXvcNyEbKV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://img.freepik.com/free-photo/blank-living-room-interior-with-copy-space_43614-928.jpg -O /content/bg001.jpg\n",
        "!wget https://img.freepik.com/free-photo/view-room-interior-with-furniture-copy-space_23-2150680550.jpg -O /content/bg002.jpg\n",
        "!wget https://img.freepik.com/premium-photo/modern-interior-design-3d-illustration_259454-1911.jpg -O /content/bg003.jpg\n",
        "!wget https://img.freepik.com/free-photo/green-plant-white-living-room-with-free-space_43614-871.jpg -O /content/bg004.jpg\n",
        "!wget https://img.freepik.com/free-photo/japandi-living-room-interior-design_53876-145502.jpg -O /content/bg005.jpg"
      ],
      "metadata": {
        "id": "7X5VdcoobLZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function"
      ],
      "metadata": {
        "id": "ejohYZMubI91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import OneFormerProcessor, OneFormerModel\n",
        "from torch import autocast\n",
        "import json\n",
        "import os\n",
        "from diffusers import StableDiffusionImg2ImgPipeline"
      ],
      "metadata": {
        "id": "xFI1r2pRQo1X"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load oneformer model\n",
        "model_id = \"shi-labs/oneformer_ade20k_swin_large\"\n",
        "processor = OneFormerProcessor.from_pretrained(model_id)\n",
        "model = OneFormerModel.from_pretrained(model_id)\n",
        "\n",
        "# load stable diffusion model\n",
        "model_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\n",
        "device = \"cuda\"\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)"
      ],
      "metadata": {
        "id": "nm61zLg1Qr1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bounding_box(mask_image):\n",
        "    # Find the indices of non-zero pixels within the mask image\n",
        "    non_zero_pixels = np.transpose(np.nonzero(mask_image))\n",
        "\n",
        "    if non_zero_pixels.size == 0:\n",
        "        # Return an empty Bounding Box if there are no non-zero pixels in the mask\n",
        "        return None\n",
        "\n",
        "    # Get x and y coordinates\n",
        "    x_coords, y_coords = non_zero_pixels[:, 0], non_zero_pixels[:, 1]\n",
        "\n",
        "    # Calculate the Bounding Box coordinates\n",
        "    min_x, min_y = np.min(x_coords), np.min(y_coords)\n",
        "    max_x, max_y = np.max(x_coords), np.max(y_coords)\n",
        "\n",
        "    return (min_x, min_y, max_x, max_y)\n",
        "\n",
        "def extract_selected_image(image_name, target_categories):\n",
        "  img_path = f\"/content/{image_name}.jpg\"\n",
        "  image = Image.open(img_path)\n",
        "\n",
        "  inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "  # you can pass them to processor for semantic postprocessing\n",
        "  predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
        "        outputs, target_sizes=[image.size[::-1]]\n",
        "  )[0]\n",
        "\n",
        "  item_list = model.config.label2id.keys()\n",
        "  include_categories = {}\n",
        "  include_categories[\"target\"] = [model.config.label2id[item] for item in item_list if any(target in item for target in target_categories)]\n",
        "\n",
        "  target_base_mask = np.zeros_like(predicted_semantic_map)\n",
        "  source_image = cv2.imread(img_path)\n",
        "  label_dict = {}\n",
        "  for sub_id in include_categories[\"target\"]:\n",
        "    label_pred_map = np.where(predicted_semantic_map == sub_id, 255, 0)\n",
        "    count = np.count_nonzero(label_pred_map == 255)\n",
        "    if count > 0:\n",
        "      bounding_box = calculate_bounding_box(label_pred_map)  # xmin, ymin, xmax, ymax\n",
        "      ymin, xmin, ymax, xmax = bounding_box\n",
        "\n",
        "      label = model.config.id2label[sub_id].split(\",\")[0].replace(\" \", \"\")\n",
        "      if label in label_dict.keys():\n",
        "        label_dict[label] += 1\n",
        "      else:\n",
        "        label_dict[label] = 0\n",
        "\n",
        "      target_image = source_image[ymin:ymax, xmin:xmax, :]\n",
        "\n",
        "    target_base_mask += label_pred_map\n",
        "\n",
        "  cv2.imwrite(\"/content/select.jpg\", target_base_mask.astype(np.uint8))\n",
        "  cv2.imwrite(f\"/content/{image_name}_mask.jpg\", target_base_mask.astype(np.uint8))\n",
        "  # display(Image.fromarray(target_base_mask.astype(np.uint8)))\n",
        "\n",
        "  # read the image\n",
        "  img = cv2.imread(\"/content/select.jpg\", 0)\n",
        "\n",
        "  # binarize the image\n",
        "  binr = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
        "\n",
        "  # define the kernel\n",
        "  kernel = np.ones((4, 4), np.uint8)\n",
        "\n",
        "  # invert the image\n",
        "  invert = cv2.bitwise_not(binr)\n",
        "\n",
        "  # erode the image\n",
        "  erosion = cv2.erode(invert, kernel,\n",
        "                      iterations=1)\n",
        "\n",
        "  erosion_invert = cv2.bitwise_not(erosion)\n",
        "  cv2.imwrite(f\"/content/{image_name}_mask.jpg\", erosion_invert)\n",
        "\n",
        "  display(Image.fromarray(erosion_invert))\n",
        "\n",
        "\n",
        "def create_alphablend(label_name, bg_name, ratio, x, y):\n",
        "  fg_img = cv2.imread(f\"/content/{label_name}.jpg\")\n",
        "  fh, fw = fg_img.shape[:2]\n",
        "  bg_img = cv2.imread(f\"/content/{bg_name}.jpg\")\n",
        "  bh, bw = bg_img.shape[:2]\n",
        "  bg_img = cv2.resize(bg_img, (int(bw * ratio), int(bh * ratio)))\n",
        "\n",
        "  mask = cv2.cvtColor(cv2.imread(f\"/content/{label_name}_mask.jpg\"), cv2.COLOR_BGR2GRAY)\n",
        "  mask = cv2.resize(mask, (fw, fh))\n",
        "\n",
        "  w = min(fg_img.shape[1], bg_img.shape[1] - x)\n",
        "  h = min(fg_img.shape[0], bg_img.shape[0] - y)\n",
        "\n",
        "  fg_roi = fg_img[:h, :w]\n",
        "  bg_roi = bg_img[y:y+h, x:x +w]\n",
        "\n",
        "  bg_roi[:] = np.where(mask[:h, :w, np.newaxis] == 0, bg_roi, fg_roi)\n",
        "\n",
        "  rgb = cv2.resize(cv2.cvtColor(bg_img, cv2.COLOR_BGR2RGB), (bw, bh))\n",
        "  display(Image.fromarray(rgb))\n",
        "  cv2.imwrite(f\"/content/{label_name}_blend.jpg\", cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "\n",
        "\n",
        "def refine_with_sd(image_name, bg_name, save_dir, strength):\n",
        "  prompt = \"\"\n",
        "  init_img = Image.open(f\"/content/{image_name}_blend.jpg\")\n",
        "\n",
        "  # パイプラインの実行\n",
        "  generator = torch.Generator(device).manual_seed(42) # 再現できるようにseedを設定\n",
        "  with torch.autocast(\"cuda\"):\n",
        "      image = pipe(prompt, image=init_img, guidance_scale=7.5, strength=strength, generator=generator).images[0]\n",
        "\n",
        "  image.save(f\"{save_dir}/{image_name}_{bg_name}.jpg\")\n",
        "  display(image)\n"
      ],
      "metadata": {
        "id": "l65z8V3bdSp5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess function"
      ],
      "metadata": {
        "id": "VOQLqyY0fHBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_name = \"room\"\n",
        "target_categories = [\"sofa\", \"table\", \"cushion\", \"rug\", \"chair\", \"computer\"]\n",
        "\n",
        "extract_selected_image(image_name, target_categories)"
      ],
      "metadata": {
        "id": "oUAjwnVDeWdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratio = 2\n",
        "x, y = 100, 100\n",
        "bg_name = \"bg005\"\n",
        "create_alphablend(image_name, bg_name, ratio, x, y)"
      ],
      "metadata": {
        "id": "r-Z72aQAe6Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/dreamboothimages\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "refine_with_sd(image_name, bg_name, save_dir, 0.3)"
      ],
      "metadata": {
        "id": "J3nYHRn3epZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning"
      ],
      "metadata": {
        "id": "nK5EEZV9gTpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/huggingface/diffusers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4MO7z1_gU0d",
        "outputId": "b515a81e-b9e0-4b12-b2a8-6c332a760ae3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 40712, done.\u001b[K\n",
            "remote: Counting objects: 100% (521/521), done.\u001b[K\n",
            "remote: Compressing objects: 100% (280/280), done.\u001b[K\n",
            "remote: Total 40712 (delta 309), reused 362 (delta 194), pack-reused 40191\u001b[K\n",
            "Receiving objects: 100% (40712/40712), 27.22 MiB | 19.52 MiB/s, done.\n",
            "Resolving deltas: 100% (30140/30140), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"/content/normal/room\", exist_ok=True)"
      ],
      "metadata": {
        "id": "BLNiJIyigaOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate.utils import write_basic_config\n",
        "write_basic_config()"
      ],
      "metadata": {
        "id": "XI-GX_C1g4em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/diffusers/examples/research_projects/multi_subject_dreambooth\n",
        "import json\n",
        "\n",
        "# here we are using parameters for prior-preservation and validation as well.\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of a <new1> room\",\n",
        "        \"class_prompt\":         \"photo of a room\",\n",
        "        \"instance_data_dir\":    \"/content/dreamboothimages/\",\n",
        "        \"class_data_dir\":       \"/content/normal/room\",\n",
        "    },\n",
        "]\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ],
      "metadata": {
        "id": "ixyJn9mhghv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "HA3fZMjAgvcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/diffusers/examples/research_projects/multi_subject_dreambooth\n",
        "!accelerate launch train_multi_subject_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=\"SG161222/Realistic_Vision_V5.1_noVAE\"  \\\n",
        "  --output_dir=\"testsinglecheckpoints\" \\\n",
        "  --train_text_encoder \\\n",
        "  --with_prior_preservation \\\n",
        "  --prior_loss_weight=1.0 \\\n",
        "  --concepts_list=\"concepts_list.json\" \\\n",
        "  --num_class_images=200 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --max_train_steps=1500 \\\n",
        "  --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
        "  --use_8bit_adam \\\n",
        "  --set_grads_to_none"
      ],
      "metadata": {
        "id": "VCP7kpBygtcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "ETCwHlrKgwIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/diffusers/examples/research_projects/multi_subject_dreambooth\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"testsinglecheckpoints\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9b4a8555638046eda8e0fbe8921646ad",
            "f3123836d8c14133afbb73a9ae8d5afa",
            "484197db8b8b4128b774d0a40f70c3ba",
            "ce356159b76d4871a9a7647e4c80f595",
            "ee772066f9c2412eb0e37cd7904a2344",
            "b821eb42a7124be0951a4f4fbba53b43",
            "25328223f7dd4470907cd7b621deecf5",
            "bc39386c6e9a4f0ba95fa94dcb2ae981",
            "ccd3a104f7f5455dae20e8ffe7d4e364",
            "533bfe864e0a4029a06e48de39486675",
            "51712b12e12b486a9e539c5e60244c86"
          ]
        },
        "id": "FIaJKDEphCZk",
        "outputId": "241c5a16-a671-4eac-d39a-a510a71cc387"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusers/examples/research_projects/multi_subject_dreambooth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b4a8555638046eda8e0fbe8921646ad"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a <new1> room, best quality, realistic, super detailed\"\n",
        "negative_prompt = \"worst quality, ugly, deformed, tiling, poor quality, picture frame, poorly drawn, mutation, duplicate\"\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(-1)\n",
        "images = pipe(\n",
        "    prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    num_inference_steps=100,\n",
        "    guidance_scale=7.5,\n",
        "    num_images_per_prompt=4\n",
        ").images\n",
        "\n",
        "\n",
        "for img in images:\n",
        "  display(img)"
      ],
      "metadata": {
        "id": "Su9l1JPnhE3G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}