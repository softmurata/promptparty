{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0YqcLIe6Mnhu2MQa9F8zS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/promptparty/blob/main/evaluation/evaluationGAIsegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FoxIPJBMn24"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ6sn-w3b7fG",
        "outputId": "83c228d2-db8e-4346-f991-89261d957a4c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/2.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.7/2.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://img.freepik.com/free-photo/picture-frame-on-a-wall-with-scandinavian-home-interior_53876-139779.jpg -O /content/room001.jpg"
      ],
      "metadata": {
        "id": "GwkUZYTvM5Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://img.freepik.com/free-psd/modern-interior-design-living-room_176382-1266.jpg -O /content/room002.jpg\n",
        "!wget https://img.freepik.com/premium-photo/living-room-with-white-couch-coffee-table-with-white-rug-floor_784625-7082.jpg -O /content/room003.jpg\n",
        "!wget https://img.freepik.com/free-photo/modern-living-room-interior-design_23-2150794674.jpg -O /content/room004.jpg\n",
        "!wget https://img.freepik.com/premium-photo/living-room-with-large-plant-white-pot_784625-7578.jpg -O /content/room005.jpg"
      ],
      "metadata": {
        "id": "oVzOooW0Qwt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py\n",
        "\n",
        "# 1085, 1086\n",
        "# after changing, please restart\n",
        "\"\"\"\n",
        "# class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n",
        "# masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n",
        "class_queries_logits = outputs.transformer_decoder_class_predictions\n",
        "masks_queries_logits = outputs.transformer_decoder_mask_predictions\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ybqKZhc-NI0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oneformer inference"
      ],
      "metadata": {
        "id": "yw0gva7BUno8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import OneFormerProcessor, OneFormerModel\n",
        "from torch import autocast\n",
        "import json\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "RZCNrVbeNYlx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_images_dir = \"/content/objects\"\n",
        "os.makedirs(object_images_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "EYCAj3vBWxiz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load oneformer model\n",
        "model_id = \"shi-labs/oneformer_ade20k_swin_large\"\n",
        "processor = OneFormerProcessor.from_pretrained(model_id)\n",
        "model = OneFormerModel.from_pretrained(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txQOOQ_nNb4J",
        "outputId": "8c09735e-658b-45a2-c2c9-767c1cd0da0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:427: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_categories = [\"sofa\", \"table\", \"cushion\", \"rug\", \"chair\"]\n",
        "sub_categories = [\"plant\", \"painting\", \"vase\"]\n",
        "\n",
        "color_list = [\n",
        "    [255, 0, 0],     # 赤\n",
        "    [0, 255, 0],     # 緑\n",
        "    [0, 0, 255],     # 青\n",
        "    [255, 255, 0],   # イエロー\n",
        "    [255, 0, 255],   # マゼンタ\n",
        "    [0, 255, 255],   # シアン\n",
        "    [128, 0, 128],   # パープル\n",
        "    [128, 128, 128], # グレー\n",
        "    [0, 128, 0],     # オリーブ\n",
        "    [128, 0, 0]      # マルーン\n",
        "]\n",
        "\n",
        "color_dict = {}\n",
        "for idx, t in enumerate(target_categories):\n",
        "  color_dict[t] = color_list[idx]\n",
        "\n",
        "def calculate_bounding_box(mask_image):\n",
        "    # Find the indices of non-zero pixels within the mask image\n",
        "    non_zero_pixels = np.transpose(np.nonzero(mask_image))\n",
        "\n",
        "    if non_zero_pixels.size == 0:\n",
        "        # Return an empty Bounding Box if there are no non-zero pixels in the mask\n",
        "        return None\n",
        "\n",
        "    # Get x and y coordinates\n",
        "    x_coords, y_coords = non_zero_pixels[:, 0], non_zero_pixels[:, 1]\n",
        "\n",
        "    # Calculate the Bounding Box coordinates\n",
        "    min_x, min_y = np.min(x_coords), np.min(y_coords)\n",
        "    max_x, max_y = np.max(x_coords), np.max(y_coords)\n",
        "\n",
        "    return (min_x, min_y, max_x, max_y)"
      ],
      "metadata": {
        "id": "5AygjTRRRpeQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mask(label_name):\n",
        "  img_path = f\"/content/{label_name}.jpg\"\n",
        "  image = Image.open(img_path)\n",
        "\n",
        "  inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "  # you can pass them to processor for semantic postprocessing\n",
        "  predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
        "        outputs, target_sizes=[image.size[::-1]]\n",
        "  )[0]\n",
        "\n",
        "  # print(np.unique(predicted_semantic_map))\n",
        "\n",
        "  item_list = model.config.label2id.keys()\n",
        "  include_categories = {}\n",
        "  include_categories[\"target\"] = [model.config.label2id[item] for item in item_list if any(target in item for target in target_categories)]\n",
        "  include_categories[\"sub\"] = [model.config.label2id[item] for item in item_list if any(target in item for target in sub_categories)]\n",
        "\n",
        "  # print(include_categories)\n",
        "\n",
        "  target_base_mask = np.zeros_like(predicted_semantic_map)\n",
        "  total_count = np.prod(target_base_mask.shape)\n",
        "  source_image = cv2.imread(img_path)\n",
        "  bh, bw = source_image.shape[:2]\n",
        "  bg_image = np.zeros_like(source_image)\n",
        "\n",
        "  subprocess.call(f\"rm -rf {object_images_dir}/target\", shell=True)\n",
        "  os.makedirs(object_images_dir + \"/target\", exist_ok=True)\n",
        "\n",
        "  label_dict = {}\n",
        "\n",
        "  result_dict = {}\n",
        "\n",
        "  for target_id in include_categories[\"target\"]:\n",
        "    label_pred_map = np.where(predicted_semantic_map == target_id, 255, 0)\n",
        "    # bboxのリストと領域の計算\n",
        "    count = np.count_nonzero(label_pred_map == 255)\n",
        "    if count > 0:\n",
        "      bounding_box = calculate_bounding_box(label_pred_map)  # xmin, ymin, xmax, ymax\n",
        "      mask_ratio = count / total_count\n",
        "      ymin, xmin, ymax, xmax = bounding_box\n",
        "\n",
        "      label = model.config.id2label[target_id].split(\",\")[0].replace(\" \", \"\")\n",
        "      if label in label_dict.keys():\n",
        "        label_dict[label] += 1\n",
        "      else:\n",
        "        label_dict[label] = 0\n",
        "\n",
        "      target_color = None\n",
        "      for cl in color_dict.keys():\n",
        "        if cl in label:\n",
        "          target_color = color_dict[cl]\n",
        "\n",
        "      if target_color is not None:\n",
        "        center = [int(xmin * 0.5 + xmax * 0.5), int(ymin * 0.5 + ymax * 0.5)]\n",
        "        cv2.circle(bg_image,\n",
        "              center=(center[0], center[1]),\n",
        "              radius=20,\n",
        "              color=(target_color[0], target_color[1], target_color[2]),\n",
        "              thickness=-1,\n",
        "              lineType=cv2.LINE_4,\n",
        "              shift=0)\n",
        "\n",
        "        item_dict = {\n",
        "          \"bbox\": [float(xmin / bw), float(ymin / bh), float(xmax / bw), float(ymax / bh)],\n",
        "          \"ratio\": float(mask_ratio),\n",
        "          \"graph\": {\n",
        "              \"label\": label,\n",
        "              \"color\": target_color,\n",
        "              \"center\": [float(center[0] / bh), float(center[1] / bw)]\n",
        "          },\n",
        "          \"height\": int(bh),\n",
        "          \"width\": int(bw)\n",
        "        }\n",
        "\n",
        "      result_dict[label] = item_dict\n",
        "\n",
        "      target_image = source_image[ymin:ymax, xmin:xmax, :]\n",
        "      cv2.imwrite(object_images_dir + f\"/target/{label}_{label_dict[label]}.jpg\", target_image)\n",
        "\n",
        "\n",
        "    target_base_mask += label_pred_map\n",
        "\n",
        "\n",
        "  cv2.imwrite(f\"/content/{label_name}_graph.jpg\", bg_image)\n",
        "  json.dump(result_dict, open(f\"/content/{label_name}.json\", \"w\"))\n",
        "  display(Image.fromarray(target_base_mask.astype(np.uint8)))\n",
        "  display(Image.fromarray(bg_image))\n",
        "\n",
        "  sub_base_mask = np.zeros_like(predicted_semantic_map)\n",
        "  subprocess.call(f\"rm -rf {object_images_dir}/sub\", shell=True)\n",
        "  os.makedirs(object_images_dir + \"/sub\", exist_ok=True)\n",
        "  label_dict = {}\n",
        "  for sub_id in include_categories[\"sub\"]:\n",
        "    label_pred_map = np.where(predicted_semantic_map == sub_id, 255, 0)\n",
        "    count = np.count_nonzero(label_pred_map == 255)\n",
        "    if count > 0:\n",
        "      bounding_box = calculate_bounding_box(label_pred_map)  # xmin, ymin, xmax, ymax\n",
        "      print(count / total_count, bounding_box)\n",
        "      mask_ratio = count / total_count\n",
        "      ymin, xmin, ymax, xmax = bounding_box\n",
        "\n",
        "      label = model.config.id2label[sub_id].split(\",\")[0].replace(\" \", \"\")\n",
        "      if label in label_dict.keys():\n",
        "        label_dict[label] += 1\n",
        "      else:\n",
        "        label_dict[label] = 0\n",
        "\n",
        "      target_image = source_image[ymin:ymax, xmin:xmax, :]\n",
        "      cv2.imwrite(object_images_dir + f\"/sub/{label}_{label_dict[label]}.jpg\", target_image)\n",
        "    sub_base_mask += label_pred_map\n",
        "\n",
        "  display(Image.fromarray(sub_base_mask.astype(np.uint8)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L4EF-TuqR0Wr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_name = \"room005\"\n",
        "get_mask(label_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4rEl82xBSsTX",
        "outputId": "1cf0306d-a442-4614-d096-e5fe0af9ea1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=626x626>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAJyCAAAAABlRXJJAAAH9klEQVR4nO3dR5ajSBRAUeiT+98yPSgnECYw8fkE9w66qypTDp4CK9R1AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACM9EGPM9zwmKTUD+s/LriLmXv4utn6o6zedvOmCn6WjeS6bmOWLtx8dzcnye45frZ/ZWWZWLukYoPmHqNglDt0t+O/xqQpu0coGOUeY1DdE/x39xO4VpolPYtCkgsMQXPpVUruvjmvuewqbT50n6tV4RVYo8us3oJ1+FNa/LhjpMus3igXc/9LD3vLo1Ki9uaDAYeJxnaS/KH0vBpNjrxaTc4wl1aryZFWs8kZ5rJqNjnNZdVuciTVcHKGuZwaTo6cJEewlpOzZE2p5eQ0l1LTyWkuo7aTIyHJEazx5CxZ82k8OfKRHMEkRzDJEUxyBJMcwSRHMMkRrPXk7AtOp/XkSEdyBJMcwSRHMMkRTHIEkxzBJEcwyRFMcgSTHMEkRzDJEUxyBJMcwSRHMMkRTHIEkxzBJEcwyRFMcgSTHMEkRzDJEezn7ieQ2dD1f/801n/8zsIPWNC3fgWFEw0cmzSi29B8cocTOD5hRLfKgvXb2XfhILo17Y9yO+f/RdNDdItssY5d9Q5s/518mOQIJjmCSW7E8rC+FyQno1xekNweNjTrkxzBJDdmmKtOchOaq01ydSh3keSmLolFccsc1v/Sn96tIrg1RrkZ55LpFbfKKHcxvW2R3JyDp3TJrcQLkosKQXBlXpDcGX35IVrFFbL5QDDJrRqch3I5yc2yo6MeyS343ZxB7nKSW6C1WiQ3T3HVSO6wyfqeSAtJ7qC+tyvuGMnN2hyyZmozzJWR3CG/ixsvWzVXRHLnWLbuJrkjFkIzzJXYeVjfDlLOKj8zbHURkrrB/Qu/rZfzeY/Dwr+zYHuUK5qMs/Ogn/k3Xm8zuf1v3L7rumFyw/MfYaEVlU7RtIRhydYW62XtNBthsy+slo3kLpyeD5o1+1YCHvTCUrBf7oB+9a+sk9yVbCMV2EjONNzH9NpmlJs4Eo0dkHvEJfeImTGc/kjXI17mrYxyB3xlZQNih63kXvmmPXbE5bfhlZNsh/ZGub4/9yHUYzdeOtDPl/aS67oTC7rCG25Epbk1YZfBOfX1pjtufHK16nisOisUlNzB+dFP/n/ZHVc22J5YtrlgvWSenixu/6/UnuUzL8hlTAqFrMsdzbbkdlVm9Ok7zTn45rCd3NmN/qHJvQZzr8lWa5GSUe5EM232tkRzJcoWrDWXjI+yscJmfa5A4brcscGqveI2V/P+/bi5F3+V4p0k4ym4OOGbntCjBefmgDY49Drr4H65PGXFPZM/1yFZf8TPH9s9N6fNA15Z5HlnJiK5UqUj1vhiTKL7Irndttpzcc11jSZ3+Yzes/tDc6saTe5q+7YDJs2JbqTN5OrO5O3+bKmuyJxc0cJs+jtDimFFc8vyJnfw4FGSmZ3kaWSUNrlU82zuyWwMpQ63Lsn5fawnv9z+/gUry5Ild6C1mcBOX0Cx6IDy1uEsl3Gclym5K68sdm5uj25+/Gnt+6DQW+RZl7tm5SfbGlS255NAluQeuLZd9oyf97pqS5Bcf/Lz9bO3rTqnd915r7qx+5NrfoY0/wJ3uj+57BRzsduTe+Qc7buHPvEMMu0keRC9HReRnPPH+FAxuaXLhWju3S5PLn6RM5+wsrM6kZz1GY74+UhndlyoHFb6btM/wcf5+ZikbR2FtmRNarJfznua2v7ruhQfFqigyTNTGvDzq7bpt0T/ZZJzsX8L1qeOdFWft3fc9W4+xlr3HKO+/mOwW9vHWNWW0O1nkvA2kiPY7wWrJdDY7/3IJksFP5Opap/9L33nuquVWLAuU1wVkiOY5Aj2/OQs/h7m+cnxMF/JGTSo6/mjnL06D/P85HiYjeQsZrnavaOcol/IgpVg68kZhbicUY5gkiOY5AgmOYJJjmBfyQ2Lf4ErbIxyFS4dIeOX2/wc69DZPceVij467eNOXGfH5kOb12ci2q4LRCxenwmK7b0mydB1e7qzJsjUocvgFIc0/PrP6Hd9OPvlDl95aSiJzgDHlxMX+1oarXTGmgoHvGzZssYpmgRbT+7geHXfMOc9kl+dM0kim1PZw1Q6eemm9Tn5PUC18+Xioutn/kReFU/RDIuun/yf1Kaz6fJMvjsY1n5I80a7gmsMS04FYOxvcgdz+8hp8R5Ux4e+6w7mtlTR5nEwC9Z3231eR2klc/f71bfkXmjXYf09hcx9hbUvUmBPcvtz6b+i0xwlyZ3IZBqd4lhK7rI2+u7fAlZwdPPJXZ2G1PjwfcCrr1yIAF/uc5QTAwGmX44Jld1wsS+Rv5vryxFMcgSTHMEkRzDJEUxyBJMcwSRHMMkRTHIEkxzBJEcwyRFMcgSTHMEkRzDJEUxyBLsjOWeiv5pRjmC3JOdKq8TzDTgAAAAAAAAAAAAAAECJ/wFXJ99NFOhH7gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=626x626>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAJyCAIAAADPTLrCAAAIDElEQVR4nO3d3VLbMBCAUYfh/V85vWAmk0KcWPbqb3XOJTMNror9saJB2wYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOu49b6A6dx3Pm4lARCDo/Zq+pJVBViUAHxUFNRn1hZgOR79751u6oMVBliIh/6e60F9sMgAq/jqfQFjCmxq+KsBMC5Z/atGBZUVYAmy+ku9/ikrQH6y+qx2+ZQVIDlZBYAwsvrQZpQ0sAJkJqs/WtZOWQHSklUACCOrW4/x0cAKkJOsAkAYWQWAMLLaaz/WPjBAQrIKAGFkFQDCyCoAhJFVAAgjqwAQRlYBIIysAkAYWQWAMLJ6W+zzAlCRrAJAGFkFgDCyuvXYj7UDDJCTrAJAGFn90XJ8NKoCpCWrABBGVh/aDJFGVYDMZPVZ7eZpKkBysvpLvfJpKkB+svpXjf5pKsASZPWl2ApqKsAqPPHfu19+BSsMsBAP/Y9Ol9XaAizHo/+gorhaVYBFCUCpvb5aSQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFZ0630BSdx3Pm59AZbisX/JXk1fstYA6XnUn1QU1GdWHCAxD/kzTjf1wboDpOTxXuZ6UB8sPUA+X70vYCaBTQ1/NQBGIKtH1aigsgIkI6uH1OufsgJkIquf1S6fsgKkIasAEEZWP2gzShpYAXKQ1Xda1k5ZARKQVQAII6u72o+PBlaA2ckqAIT57n0BwA7HDcKE3KCvddyP9U+yOscNwsxMqzCME9/N/fwRcYVh+NkqjOHKDsndf3iDUcgq9BYVRWWFAcgqdOW4QchFVqEfxw1COrIKnThuEDKSVejBcYOQlKwCQBhZfa3X+wC9/3AJjhuEvGQV2nLcIKQmqwAQRlZ3td+PtQOcn+MGITtZBYAwsvpOy/HRqAqQgKxCK732Y+0DQ0Oy+kGbIdKoCpCDrH5Wu3maCpCGrB5Sr3yaCpCJrB5Vo3+aCpCMrBaIraCmAuQjq2VuQTnUVICUZPWMK1GMCjMAA/rufQGz+klj0RsC1RQgPY/6GHt9tb78p8tvZvBVCA2ZVmN4cAGw+dkqAASSVWjIcYOQnawCQBhZhbYcNwipySoAhJFVaM5xg5CXrEIPjhuEpGQVOnHcIGQkq9CP4wYhHVmFrhw3CLm4C2EM139dsLsZBuBGhGGcLqv7GIbhdoTBOG4QZuamhFE5bhAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYBrZj5hytBYADSXNi4OgAeghXVKKgvos3UoA0F6umJxu6kOu9QCgsSwZuR7UhyxLAkB7X70vIEJgU8NfDYCVzJ/VGhVUVgBOmTyr9fqnrACUmzmrtcunrAAUmjmrADCYabPaZpQ0sAJQYs6stqydsgJw2JxZBYAhTZjV9uOjgRWAYybMKgCMSlYBIMxsWe21H2sfGIADZssqAAxMVgEgjKwCQBhZBYAwsgoAYWQVAMLIKgCEkVUACDNbVm+LfV4ApjJbVgFgYLIKAGEmzGr7/Vg7wAAcM2FWAWBUc2a15fhoVAXgsDmzCgBDmjarbYZIoyoAJabN6la/eZoKQKGZs7rVLJ+mAlBu8qxudfqnqQCcMn9Wt+gKaioAZ+VqyP3yK+RaDwAaS5eR02VNtxIAtJc0JkVxTboGALSXPSl7fc3+9wYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgVv8AVcpoAZYI5SYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06129489940695526 (204, 304, 440, 472)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=626x626>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create network similarity score"
      ],
      "metadata": {
        "id": "IAY5doUHMgSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_dist_matrix(label_name):\n",
        "  result_dict = json.load(open(f\"/content/{label_name}.json\"))\n",
        "\n",
        "  point_dict = {}\n",
        "  for key in target_categories:\n",
        "    if key in result_dict.keys():\n",
        "      point_dict[key] = result_dict[key][\"graph\"]['center']\n",
        "    else:\n",
        "      point_dict[key] = [1, 1]\n",
        "\n",
        "\n",
        "  distance_dict = {}\n",
        "  dist_matrix = []\n",
        "  for key in target_categories:\n",
        "    distances = []\n",
        "    for ki in target_categories:\n",
        "      dis = np.sqrt((point_dict[key][0] - point_dict[ki][0]) ** 2 + (point_dict[key][1] - point_dict[ki][1]) ** 2)\n",
        "      distances.append(dis)\n",
        "\n",
        "    if len(distances) <= 10:\n",
        "      pad_dis = [0 for _ in range(10 - len(distances))]\n",
        "      distances.extend(pad_dis)\n",
        "\n",
        "    distance_dict[key] = distances\n",
        "    dist_matrix.append(distances)\n",
        "\n",
        "  return dist_matrix"
      ],
      "metadata": {
        "id": "oMJN1ckHMiFj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = [\n",
        "    \"room001\",\"room002\", \"room003\", \"room004\", \"room005\"\n",
        "]\n",
        "\n",
        "label_dist_matrix_dict = {}\n",
        "\n",
        "for lname in label_names:\n",
        "  label_dist_matrix_dict[lname] = calculate_dist_matrix(lname)"
      ],
      "metadata": {
        "id": "UxFO-NiMN6Lt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarities = cosine_similarity(label_dist_matrix_dict[\"room001\"], label_dist_matrix_dict[\"room002\"])\n",
        "# print(similarities)\n",
        "\n",
        "from scipy.spatial import distance\n",
        "\n",
        "cs_list = []\n",
        "source_name = \"room001\"\n",
        "for target_name in [\"room002\", \"room003\", \"room004\", \"room005\"]:\n",
        "  v = 1 - distance.cosine(np.array(label_dist_matrix_dict[source_name]).reshape(-1), np.array(label_dist_matrix_dict[target_name]).reshape(-1))\n",
        "  cs_list.append(v)\n",
        "\n",
        "normalize_cs_list = [(c - min(cs_list))/(max(cs_list) - min(cs_list)) for c in cs_list]\n",
        "mix_cs_list = [0.2 * ncs + 0.8 * cs for ncs, cs in zip(normalize_cs_list, cs_list)]\n",
        "print(mix_cs_list)"
      ],
      "metadata": {
        "id": "NSmfLOYvQd04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Object similarity score"
      ],
      "metadata": {
        "id": "xt7aM-VLilgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get object image"
      ],
      "metadata": {
        "id": "avbP9eBxaped"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "transform = [\n",
        "    A.Resize(256,256,p=1),\n",
        "    ToTensorV2(p=1.0)\n",
        "]\n",
        "transform_infer = A.Compose(transform)\n",
        "effnet = timm.create_model('efficientnetv2_s')"
      ],
      "metadata": {
        "id": "8SELEizTcIPm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_item_features(label_name):\n",
        "  object_image_dict = {}\n",
        "  result_dict = json.load(open(f\"/content/{label_name}.json\"))\n",
        "  base_img = cv2.imread(f\"/content/{label_name}.jpg\")\n",
        "\n",
        "  bh, bw = base_img.shape[:2]\n",
        "\n",
        "  for key in target_categories:\n",
        "    if key in result_dict.keys():\n",
        "      value = result_dict[key]\n",
        "      bbox = value['bbox']\n",
        "      xmin, ymin, xmax, ymax = int(bbox[0] * bw), int(bbox[1] * bh), int(bbox[2] * bw), int(bbox[3] * bh)\n",
        "      crop_img = base_img[ymin:ymax, xmin:xmax, :]\n",
        "      object_image_dict[key] = crop_img\n",
        "    else:\n",
        "      object_image_dict[key] = np.zeros((256, 256, 3))\n",
        "\n",
        "\n",
        "  features_dict = {}\n",
        "  for key in object_image_dict.keys():\n",
        "    input_img = object_image_dict[key]\n",
        "    img = input_img[:, :, ::-1].astype(np.float32)\n",
        "    input_img = transform_infer(image=img)[\"image\"]\n",
        "    features = effnet.forward_features(input_img.unsqueeze(0))\n",
        "    features = features.detach().cpu().view(-1).numpy()\n",
        "    features_dict[key] = features\n",
        "\n",
        "  return features_dict"
      ],
      "metadata": {
        "id": "qbnc0sjMcqrc"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = [\n",
        "    \"room001\", \"room002\", \"room003\", \"room004\", \"room005\"\n",
        "]\n",
        "\n",
        "label_feature_dict = {}\n",
        "for lname in label_names:\n",
        "  fe_dict = get_item_features(lname)\n",
        "  label_feature_dict[lname] = fe_dict"
      ],
      "metadata": {
        "id": "Mv6N3R3wep9N"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cs_list = []\n",
        "\n",
        "source_name = \"room001\"\n",
        "for target_name in [\"room002\", \"room003\", \"room004\", \"room005\"]:\n",
        "\n",
        "  source_feature_dict = label_feature_dict[source_name]\n",
        "  target_feature_dict = label_feature_dict[target_name]\n",
        "\n",
        "  cos_sim_value = 0\n",
        "\n",
        "  for key in target_feature_dict.keys():\n",
        "    cs = 1 - distance.cosine(source_feature_dict[key], target_feature_dict[key])\n",
        "    cos_sim_value += cs\n",
        "\n",
        "  v = cos_sim_value / len(target_feature_dict.keys())\n",
        "  cs_list.append(v)\n",
        "\n",
        "normalize_cs_list = [(c - min(cs_list))/(max(cs_list) - min(cs_list)) for c in cs_list]\n",
        "mix_cs_list = [0.2 * ncs + 0.8 * cs for ncs, cs in zip(normalize_cs_list, cs_list)]\n",
        "print(mix_cs_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu7Yi5RtgM2W",
        "outputId": "745dc956-50fd-4727-8fe9-1ce2a551afa6"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2726347269151851, 0.2519580855965614, 0.2821250236039688, 0.8361088559031487]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/spatial/distance.py:636: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create segmentation similarity score"
      ],
      "metadata": {
        "id": "GQYtbWCgiwHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_segmentation_score(label_name):\n",
        "  segmentation_ratio_dict = {}\n",
        "  result_dict = json.load(open(f\"/content/{label_name}.json\"))\n",
        "  for key in target_categories:\n",
        "    if key in result_dict.keys():\n",
        "      mask_ratio = result_dict[key][\"ratio\"]\n",
        "      segmentation_ratio_dict[key] = [mask_ratio]\n",
        "    else:\n",
        "      segmentation_ratio_dict[key] = [0]\n",
        "\n",
        "  return segmentation_ratio_dict"
      ],
      "metadata": {
        "id": "lKYGg-GDiyf8"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_segmentation_ratio_dict = {}\n",
        "\n",
        "for lname in label_names:\n",
        "  label_segmentation_ratio_dict[lname] = get_segmentation_score(lname)"
      ],
      "metadata": {
        "id": "ARh_jYHqkUlC"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cs_list = []\n",
        "\n",
        "source_name = \"room001\"\n",
        "for target_name in [\"room002\", \"room003\", \"room004\", \"room005\"]:\n",
        "  cos_sim_value = 0\n",
        "\n",
        "  source_segmentation_ratio_dict = label_segmentation_ratio_dict[source_name]\n",
        "  target_segmentation_ratio_dict = label_segmentation_ratio_dict[target_name]\n",
        "\n",
        "  for key in target_segmentation_ratio_dict.keys():\n",
        "    cs = abs(source_segmentation_ratio_dict[key][0] - target_segmentation_ratio_dict[key][0])\n",
        "    cos_sim_value += cs\n",
        "\n",
        "  v = 1 - cos_sim_value / len(target_segmentation_ratio_dict.keys())\n",
        "  cs_list.append(v)\n",
        "\n",
        "normalize_cs_list = [(c - min(cs_list))/(max(cs_list) - min(cs_list)) for c in cs_list]\n",
        "mix_cs_list = [0.2 * ncs + 0.8 * cs for ncs, cs in zip(normalize_cs_list, cs_list)]\n",
        "print(mix_cs_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TaRx4xukiUr",
        "outputId": "d5e3e226-1c1b-4472-bda4-e37de742bf7a"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9807990725372278, 0.9055899643359138, 0.9875023942507337, 0.7821765280668116]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create beautify score"
      ],
      "metadata": {
        "id": "aFmFDCzJqSv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vAyx8qOeqVJn"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_model = 'koniq' #@param ['spaq', 'koniq', 'paq2piq', 'ava']\n",
        "\n",
        "NAME_TO_HANDLE = {\n",
        "    # Model trained on SPAQ dataset: https://github.com/h4nwei/SPAQ\n",
        "    'spaq': 'https://tfhub.dev/google/musiq/spaq/1',\n",
        "\n",
        "    # Model trained on KonIQ-10K dataset: http://database.mmsp-kn.de/koniq-10k-database.html\n",
        "    'koniq': 'https://tfhub.dev/google/musiq/koniq-10k/1',\n",
        "\n",
        "    # Model trained on PaQ2PiQ dataset: https://github.com/baidut/PaQ-2-PiQ\n",
        "    'paq2piq': 'https://tfhub.dev/google/musiq/paq2piq/1',\n",
        "\n",
        "    # Model trained on AVA dataset: https://ieeexplore.ieee.org/document/6247954\n",
        "    'ava': 'https://tfhub.dev/google/musiq/ava/1',\n",
        "}\n",
        "\n",
        "model_handle = NAME_TO_HANDLE[selected_model]\n",
        "beautify_model = hub.load(model_handle)\n",
        "predict_fn = beautify_model.signatures['serving_default']\n",
        "\n",
        "print(f'loaded model {selected_model} ({model_handle})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDzrw8h_qXgA",
        "outputId": "06bd5bce-ea65-4333-8cbb-2407e3a05f82"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model koniq (https://tfhub.dev/google/musiq/koniq-10k/1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "file_path = \"/content/room001.jpg\"\n",
        "img = Image.open(file_path, mode='r')\n",
        "resized_img = img  # .resize((1024, 1024))  # 32x32に変形\n",
        "\n",
        "image_bytes = io.BytesIO()\n",
        "resized_img.save(image_bytes, format='PNG')\n",
        "image_bytes = image_bytes.getvalue()  # これが bytes\n",
        "\n",
        "prediction = predict_fn(tf.constant(image_bytes))\n",
        "beautify_score = prediction[\"output_0\"].numpy() / 100\n",
        "print(\"beautify score: \", beautify_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiNcxBInqelX",
        "outputId": "7ee0a7f4-b6b6-4c95-eeb6-c3c680f02650"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted MOS:  0.7462981414794921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Unsemble score"
      ],
      "metadata": {
        "id": "E3GKkNEWp23Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# network similarity score + object similarity score + segmentation score + beautify score"
      ],
      "metadata": {
        "id": "0cFpF-w9p43b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}