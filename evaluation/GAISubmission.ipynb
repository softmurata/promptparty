{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOl06ViGC2zwVKSDL+BJYFe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/promptparty/blob/main/evaluation/GAISubmission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Demo data"
      ],
      "metadata": {
        "id": "P0ywOVP6r64D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get source data\n",
        "!wget https://img.freepik.com/free-photo/picture-frame-on-a-wall-with-scandinavian-home-interior_53876-139779.jpg -O /content/room001.jpg\n",
        "\n",
        "# Get target data\n",
        "!wget https://img.freepik.com/free-psd/modern-interior-design-living-room_176382-1266.jpg -O /content/room002.jpg\n",
        "!wget https://img.freepik.com/premium-photo/living-room-with-white-couch-coffee-table-with-white-rug-floor_784625-7082.jpg -O /content/room003.jpg\n",
        "!wget https://img.freepik.com/free-photo/modern-living-room-interior-design_23-2150794674.jpg -O /content/room004.jpg\n",
        "!wget https://img.freepik.com/premium-photo/living-room-with-large-plant-white-pot_784625-7578.jpg -O /content/room005.jpg"
      ],
      "metadata": {
        "id": "5J5Qvizar761"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "OeK4OuearvfR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dddYsvb-roxc"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes diffusers timm open_clip_torch sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix point"
      ],
      "metadata": {
        "id": "M2pj8qrZr93q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# /usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py\n",
        "\n",
        "# 1085, 1086\n",
        "# after changing, please restart\n",
        "\"\"\"\n",
        "# class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n",
        "# masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n",
        "class_queries_logits = outputs.transformer_decoder_class_predictions\n",
        "masks_queries_logits = outputs.transformer_decoder_mask_predictions\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OS5tBsaFr4p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function"
      ],
      "metadata": {
        "id": "Z5mtsdVzZeR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from transformers import OneFormerProcessor, OneFormerModel\n",
        "from torch import autocast\n",
        "import json\n",
        "import subprocess\n",
        "from scipy.spatial import distance\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "import open_clip\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "jpNpKvLEsBrU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load oneformer model\n",
        "model_id = \"shi-labs/oneformer_ade20k_swin_large\"\n",
        "processor = OneFormerProcessor.from_pretrained(model_id)\n",
        "model = OneFormerModel.from_pretrained(model_id)\n",
        "\n",
        "# load efficient net model\n",
        "transform = [\n",
        "    A.Resize(256,256,p=1),\n",
        "    ToTensorV2(p=1.0)\n",
        "]\n",
        "transform_infer = A.Compose(transform)\n",
        "effnet = timm.create_model('efficientnetv2_s')\n",
        "\n",
        "\n",
        "# load text embeddong model\n",
        "en_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# image embedding model\n",
        "coca_model, _, coca_transform = open_clip.create_model_and_transforms(\n",
        "  model_name=\"coca_ViT-L-14\",\n",
        "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZwrTlMhssXPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_images_dir = \"/content/objects\"\n",
        "os.makedirs(object_images_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "o0TSZPa8tDRI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_categories = [\"sofa\", \"table\", \"cushion\", \"rug\", \"chair\"]\n",
        "sub_categories = [\"plant\", \"painting\", \"vase\"]\n",
        "\n",
        "color_list = [\n",
        "    [255, 0, 0],     # 赤\n",
        "    [0, 255, 0],     # 緑\n",
        "    [0, 0, 255],     # 青\n",
        "    [255, 255, 0],   # イエロー\n",
        "    [255, 0, 255],   # マゼンタ\n",
        "    [0, 255, 255],   # シアン\n",
        "    [128, 0, 128],   # パープル\n",
        "    [128, 128, 128], # グレー\n",
        "    [0, 128, 0],     # オリーブ\n",
        "    [128, 0, 0]      # マルーン\n",
        "]\n",
        "\n",
        "color_dict = {}\n",
        "for idx, t in enumerate(target_categories):\n",
        "  color_dict[t] = color_list[idx]"
      ],
      "metadata": {
        "id": "hJPCLXfVvWHp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Utils function\n",
        "def calculate_bounding_box(mask_image):\n",
        "    # Find the indices of non-zero pixels within the mask image\n",
        "    non_zero_pixels = np.transpose(np.nonzero(mask_image))\n",
        "\n",
        "    if non_zero_pixels.size == 0:\n",
        "        # Return an empty Bounding Box if there are no non-zero pixels in the mask\n",
        "        return None\n",
        "\n",
        "    # Get x and y coordinates\n",
        "    x_coords, y_coords = non_zero_pixels[:, 0], non_zero_pixels[:, 1]\n",
        "\n",
        "    # Calculate the Bounding Box coordinates\n",
        "    min_x, min_y = np.min(x_coords), np.min(y_coords)\n",
        "    max_x, max_y = np.max(x_coords), np.max(y_coords)\n",
        "\n",
        "    return (min_x, min_y, max_x, max_y)\n",
        "\n",
        "\n",
        "def get_mask(label_name):\n",
        "  img_path = f\"/content/{label_name}.jpg\"\n",
        "  image = Image.open(img_path)\n",
        "\n",
        "  inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "  # you can pass them to processor for semantic postprocessing\n",
        "  predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
        "        outputs, target_sizes=[image.size[::-1]]\n",
        "  )[0]\n",
        "\n",
        "  # print(np.unique(predicted_semantic_map))\n",
        "\n",
        "  item_list = model.config.label2id.keys()\n",
        "  include_categories = {}\n",
        "  include_categories[\"target\"] = [model.config.label2id[item] for item in item_list if any(target in item for target in target_categories)]\n",
        "  include_categories[\"sub\"] = [model.config.label2id[item] for item in item_list if any(target in item for target in sub_categories)]\n",
        "\n",
        "  # print(include_categories)\n",
        "\n",
        "  target_base_mask = np.zeros_like(predicted_semantic_map)\n",
        "  total_count = np.prod(target_base_mask.shape)\n",
        "  source_image = cv2.imread(img_path)\n",
        "  bh, bw = source_image.shape[:2]\n",
        "  bg_image = np.zeros_like(source_image)\n",
        "\n",
        "  subprocess.call(f\"rm -rf {object_images_dir}/target\", shell=True)\n",
        "  os.makedirs(object_images_dir + \"/target\", exist_ok=True)\n",
        "\n",
        "  label_dict = {}\n",
        "\n",
        "  result_dict = {}\n",
        "\n",
        "  for target_id in include_categories[\"target\"]:\n",
        "    label_pred_map = np.where(predicted_semantic_map == target_id, 255, 0)\n",
        "    # bboxのリストと領域の計算\n",
        "    count = np.count_nonzero(label_pred_map == 255)\n",
        "    if count > 0:\n",
        "      bounding_box = calculate_bounding_box(label_pred_map)  # xmin, ymin, xmax, ymax\n",
        "      mask_ratio = count / total_count\n",
        "      ymin, xmin, ymax, xmax = bounding_box\n",
        "\n",
        "      label = model.config.id2label[target_id].split(\",\")[0].replace(\" \", \"\")\n",
        "      if label in label_dict.keys():\n",
        "        label_dict[label] += 1\n",
        "      else:\n",
        "        label_dict[label] = 0\n",
        "\n",
        "      target_color = None\n",
        "      for cl in color_dict.keys():\n",
        "        if cl in label:\n",
        "          target_color = color_dict[cl]\n",
        "\n",
        "      if target_color is not None:\n",
        "        center = [int(xmin * 0.5 + xmax * 0.5), int(ymin * 0.5 + ymax * 0.5)]\n",
        "        cv2.circle(bg_image,\n",
        "              center=(center[0], center[1]),\n",
        "              radius=20,\n",
        "              color=(target_color[0], target_color[1], target_color[2]),\n",
        "              thickness=-1,\n",
        "              lineType=cv2.LINE_4,\n",
        "              shift=0)\n",
        "\n",
        "        item_dict = {\n",
        "          \"bbox\": [float(xmin / bw), float(ymin / bh), float(xmax / bw), float(ymax / bh)],\n",
        "          \"ratio\": float(mask_ratio),\n",
        "          \"graph\": {\n",
        "              \"label\": label,\n",
        "              \"color\": target_color,\n",
        "              \"center\": [float(center[0] / bh), float(center[1] / bw)]\n",
        "          },\n",
        "          \"height\": int(bh),\n",
        "          \"width\": int(bw)\n",
        "        }\n",
        "\n",
        "      result_dict[label] = item_dict\n",
        "\n",
        "      target_image = source_image[ymin:ymax, xmin:xmax, :]\n",
        "      cv2.imwrite(object_images_dir + f\"/target/{label}_{label_dict[label]}.jpg\", target_image)\n",
        "\n",
        "\n",
        "    target_base_mask += label_pred_map\n",
        "\n",
        "\n",
        "  cv2.imwrite(f\"/content/{label_name}_graph.jpg\", bg_image)\n",
        "  json.dump(result_dict, open(f\"/content/{label_name}.json\", \"w\"))\n",
        "  display(Image.fromarray(target_base_mask.astype(np.uint8)))\n",
        "  display(Image.fromarray(bg_image))\n",
        "\n",
        "  sub_base_mask = np.zeros_like(predicted_semantic_map)\n",
        "  subprocess.call(f\"rm -rf {object_images_dir}/sub\", shell=True)\n",
        "  os.makedirs(object_images_dir + \"/sub\", exist_ok=True)\n",
        "  label_dict = {}\n",
        "  for sub_id in include_categories[\"sub\"]:\n",
        "    label_pred_map = np.where(predicted_semantic_map == sub_id, 255, 0)\n",
        "    count = np.count_nonzero(label_pred_map == 255)\n",
        "    if count > 0:\n",
        "      bounding_box = calculate_bounding_box(label_pred_map)  # xmin, ymin, xmax, ymax\n",
        "      print(count / total_count, bounding_box)\n",
        "      mask_ratio = count / total_count\n",
        "      ymin, xmin, ymax, xmax = bounding_box\n",
        "\n",
        "      label = model.config.id2label[sub_id].split(\",\")[0].replace(\" \", \"\")\n",
        "      if label in label_dict.keys():\n",
        "        label_dict[label] += 1\n",
        "      else:\n",
        "        label_dict[label] = 0\n",
        "\n",
        "      target_image = source_image[ymin:ymax, xmin:xmax, :]\n",
        "      cv2.imwrite(object_images_dir + f\"/sub/{label}_{label_dict[label]}.jpg\", target_image)\n",
        "    sub_base_mask += label_pred_map\n",
        "\n",
        "  display(Image.fromarray(sub_base_mask.astype(np.uint8)))\n",
        "\n",
        "def calculate_dist_matrix(label_name):\n",
        "  result_dict = json.load(open(f\"/content/{label_name}.json\"))\n",
        "\n",
        "  point_dict = {}\n",
        "  for key in target_categories:\n",
        "    if key in result_dict.keys():\n",
        "      point_dict[key] = result_dict[key][\"graph\"]['center']\n",
        "    else:\n",
        "      point_dict[key] = [1, 1]\n",
        "\n",
        "\n",
        "  distance_dict = {}\n",
        "  dist_matrix = []\n",
        "  for key in target_categories:\n",
        "    distances = []\n",
        "    for ki in target_categories:\n",
        "      dis = np.sqrt((point_dict[key][0] - point_dict[ki][0]) ** 2 + (point_dict[key][1] - point_dict[ki][1]) ** 2)\n",
        "      distances.append(dis)\n",
        "\n",
        "    if len(distances) <= 10:\n",
        "      pad_dis = [0 for _ in range(10 - len(distances))]\n",
        "      distances.extend(pad_dis)\n",
        "\n",
        "    distance_dict[key] = distances\n",
        "    dist_matrix.append(distances)\n",
        "\n",
        "  return dist_matrix\n",
        "\n",
        "\n",
        "\n",
        "def get_item_features(label_name):\n",
        "  object_image_dict = {}\n",
        "  result_dict = json.load(open(f\"/content/{label_name}.json\"))\n",
        "  base_img = cv2.imread(f\"/content/{label_name}.jpg\")\n",
        "\n",
        "  bh, bw = base_img.shape[:2]\n",
        "\n",
        "  for key in target_categories:\n",
        "    if key in result_dict.keys():\n",
        "      value = result_dict[key]\n",
        "      bbox = value['bbox']\n",
        "      xmin, ymin, xmax, ymax = int(bbox[0] * bw), int(bbox[1] * bh), int(bbox[2] * bw), int(bbox[3] * bh)\n",
        "      crop_img = base_img[ymin:ymax, xmin:xmax, :]\n",
        "      object_image_dict[key] = crop_img\n",
        "    else:\n",
        "      object_image_dict[key] = np.zeros((256, 256, 3))\n",
        "\n",
        "\n",
        "  features_dict = {}\n",
        "  for key in object_image_dict.keys():\n",
        "    input_img = object_image_dict[key]\n",
        "    img = input_img[:, :, ::-1].astype(np.float32)\n",
        "    input_img = transform_infer(image=img)[\"image\"]\n",
        "    features = effnet.forward_features(input_img.unsqueeze(0))\n",
        "    features = features.detach().cpu().view(-1).numpy()\n",
        "    features_dict[key] = features\n",
        "\n",
        "  return features_dict\n",
        "\n",
        "\n",
        "def get_segmentation_score(label_name):\n",
        "  segmentation_ratio_dict = {}\n",
        "  result_dict = json.load(open(f\"/content/{label_name}.json\"))\n",
        "  for key in target_categories:\n",
        "    if key in result_dict.keys():\n",
        "      mask_ratio = result_dict[key][\"ratio\"]\n",
        "      segmentation_ratio_dict[key] = [mask_ratio]\n",
        "    else:\n",
        "      segmentation_ratio_dict[key] = [0]\n",
        "\n",
        "  return segmentation_ratio_dict\n",
        "\n",
        "def get_clip_features(label_name):\n",
        "\n",
        "  im = Image.open(f\"/content/{label_name}.jpg\").convert(\"RGB\")\n",
        "  im = coca_transform(im).unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "      image_features = coca_model.encode_image(im)\n",
        "      generated = coca_model.generate(im)\n",
        "\n",
        "  text = open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\")\n",
        "\n",
        "  return image_features.detach().cpu().numpy(), text\n",
        "\n"
      ],
      "metadata": {
        "id": "j89BFVWvsp1z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create target data"
      ],
      "metadata": {
        "id": "SLTpE2KAtKxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_label_name = \"room001\"\n",
        "target_label_name = \"room002\""
      ],
      "metadata": {
        "id": "El0B-5gVg0KZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create source data\n",
        "get_mask(source_label_name)\n",
        "\n",
        "# create target data\n",
        "get_mask(target_label_name)"
      ],
      "metadata": {
        "id": "qNuWp5DGtG7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate score"
      ],
      "metadata": {
        "id": "0D0vk0-Jtbcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = [source_label_name, target_label_name]\n",
        "\n",
        "## Network Similarity\n",
        "label_dist_matrix_dict = {}\n",
        "\n",
        "for lname in label_names:\n",
        "  label_dist_matrix_dict[lname] = calculate_dist_matrix(lname)\n",
        "\n",
        "nsv = 1 - distance.cosine(np.array(label_dist_matrix_dict[source_label_name]).reshape(-1), np.array(label_dist_matrix_dict[target_label_name]).reshape(-1))\n",
        "print(\"Network Similarity Score: \",nsv)\n",
        "\n",
        "## Object Similarity\n",
        "label_feature_dict = {}\n",
        "for lname in label_names:\n",
        "  fe_dict = get_item_features(lname)\n",
        "  label_feature_dict[lname] = fe_dict\n",
        "\n",
        "source_feature_dict = label_feature_dict[source_label_name]\n",
        "target_feature_dict = label_feature_dict[target_label_name]\n",
        "\n",
        "cos_sim_value = 0\n",
        "\n",
        "for key in target_feature_dict.keys():\n",
        "  cs = 1 - distance.cosine(source_feature_dict[key], target_feature_dict[key])\n",
        "  cos_sim_value += cs\n",
        "\n",
        "osv = cos_sim_value / len(target_feature_dict.keys())\n",
        "print(\"Object Similarity Score: \",osv)\n",
        "\n",
        "## Segmentation Similaerity\n",
        "label_segmentation_ratio_dict = {}\n",
        "\n",
        "for lname in label_names:\n",
        "  label_segmentation_ratio_dict[lname] = get_segmentation_score(lname)\n",
        "\n",
        "cos_sim_value = 0\n",
        "\n",
        "source_segmentation_ratio_dict = label_segmentation_ratio_dict[source_label_name]\n",
        "target_segmentation_ratio_dict = label_segmentation_ratio_dict[target_label_name]\n",
        "\n",
        "for key in target_segmentation_ratio_dict.keys():\n",
        "  cs = abs(source_segmentation_ratio_dict[key][0] - target_segmentation_ratio_dict[key][0])\n",
        "  cos_sim_value += cs\n",
        "\n",
        "ssv = 1 - cos_sim_value / len(target_segmentation_ratio_dict.keys())\n",
        "print(\"Segmentation Similarity Score: \",ssv)\n",
        "\n",
        "##  Overview score\n",
        "source_img_fea, source_gen_text = get_clip_features(source_label_name)\n",
        "target_img_fea, target_gen_text = get_clip_features(target_label_name)\n",
        "# print(source_gen_text, target_gen_text)\n",
        "\n",
        "source_sentences = [source_gen_text]\n",
        "target_sentences = [target_gen_text]\n",
        "\n",
        "source_embeddings = en_model.encode(source_sentences)\n",
        "target_embeddings = en_model.encode(target_sentences)\n",
        "\n",
        "tv = 1 - distance.cosine(source_embeddings.reshape(-1), target_embeddings.reshape(-1))\n",
        "sv = 1 - distance.cosine(source_img_fea.reshape(-1), target_img_fea.reshape(-1))\n",
        "print(\"text similarity score: \", tv)\n",
        "print(\"image similarity score: \", sv)"
      ],
      "metadata": {
        "id": "_0frTC-9tc1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0695956-4b9d-4e76-860e-ab890fe1a190"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Similarity Score:  0.7761646515989753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/spatial/distance.py:636: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object Similarity Score:  0.5026776701211929\n",
            "Segmentation Similarity Score:  0.9841606493102735\n",
            "text similarity score:  0.6492205262184143 image similarity score:  0.8205609917640686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 以下の得点の重み係数和で最終的なスコアを算出\n",
        "# Network Similarity Score:  0.7761646515989753\n",
        "# Object Similarity Score:  0.5026776701211929\n",
        "# Segmentation Similarity Score:  0.9841606493102735\n",
        "# text similarity score:  0.6492205262184143\n",
        "# image similarity score:  0.8205609917640686\n",
        "# beautify score:  0.869676470396118"
      ],
      "metadata": {
        "id": "0IgA67_HiYy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beautify Score\n",
        "\n",
        "Please restart colab before running the following code"
      ],
      "metadata": {
        "id": "2dhUbMFoyO1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import io"
      ],
      "metadata": {
        "id": "V-Erq-RRyRbL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load musiq model\n",
        "selected_model = 'koniq' # ['spaq', 'koniq', 'paq2piq', 'ava']\n",
        "\n",
        "NAME_TO_HANDLE = {\n",
        "    # Model trained on SPAQ dataset: https://github.com/h4nwei/SPAQ\n",
        "    'spaq': 'https://tfhub.dev/google/musiq/spaq/1',\n",
        "\n",
        "    # Model trained on KonIQ-10K dataset: http://database.mmsp-kn.de/koniq-10k-database.html\n",
        "    'koniq': 'https://tfhub.dev/google/musiq/koniq-10k/1',\n",
        "\n",
        "    # Model trained on PaQ2PiQ dataset: https://github.com/baidut/PaQ-2-PiQ\n",
        "    'paq2piq': 'https://tfhub.dev/google/musiq/paq2piq/1',\n",
        "\n",
        "    # Model trained on AVA dataset: https://ieeexplore.ieee.org/document/6247954\n",
        "    'ava': 'https://tfhub.dev/google/musiq/ava/1',\n",
        "}\n",
        "\n",
        "model_handle = NAME_TO_HANDLE[selected_model]\n",
        "beautify_model = hub.load(model_handle)\n",
        "predict_fn = beautify_model.signatures['serving_default']\n",
        "\n",
        "def calculate_beautify_score(label_name):\n",
        "  file_path = f\"/content/{label_name}.jpg\"\n",
        "  img = Image.open(file_path, mode='r')\n",
        "  resized_img = img.resize((1024, 1024))  # 32x32に変形\n",
        "\n",
        "  image_bytes = io.BytesIO()\n",
        "  resized_img.save(image_bytes, format='PNG')\n",
        "  image_bytes = image_bytes.getvalue()  # これが bytes\n",
        "\n",
        "  prediction = predict_fn(tf.constant(image_bytes))\n",
        "  beautify_score = prediction[\"output_0\"].numpy() / 100\n",
        "\n",
        "  return beautify_score"
      ],
      "metadata": {
        "id": "pr89216_w74f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Beautify score\n",
        "source_label_name = \"room001\"\n",
        "target_label_name = \"room002\"\n",
        "source_beautify_score = calculate_beautify_score(source_label_name)\n",
        "target_beautify_score = calculate_beautify_score(target_label_name)\n",
        "bsv = source_beautify_score / target_beautify_score\n",
        "print(\"beautify score: \", bsv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHheDQcRwl_f",
        "outputId": "8b13e3e0-873b-46d3-f0bd-1ac4b7a3188c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beautify score:  0.869676470396118\n"
          ]
        }
      ]
    }
  ]
}